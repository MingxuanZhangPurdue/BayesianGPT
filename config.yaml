SparsityScheduler:
  name: CubicSparsityScheduler
  params:
    initial_sparsity: 0.1
    final_sparsity: 0.8
    pruning_start_step: 0.1
    pruning_end_step: 0.9
    pruning_interval: 0.01

PriorScheduler:
  name: ConstantPriorScheduler
  params:
    lambda_mix: 1e-4
    sigma0: 1e-10
    sigma1: 0.01

Pruner:
  name: UnstructuredBayesianPruner
  params:
    target_modules:
      - transformer.h.*.mlp.c_fc.weight
      - transformer.h.*.mlp.c_proj.weight
      - transformer.h.*.attn.c_attn.weight
      - transformer.h.*.attn.c_proj.weight

Arguments:
  trust_remote_code: true
  dataset_name: HuggingFaceFW/fineweb
  dataset_config_name: sample-10BT
  validation_split_percentage: 5
  model_name_or_path: openai-community/gpt2
  per_device_train_batch_size: 32
  learning_rate: 1e-5
  num_train_epochs: 1
  gradient_accumulation_steps: 1
  num_warmup_steps: 500
  seed: 42
  block_size: 1024
  preprocessing_num_workers: 64
  output_dir: ./output