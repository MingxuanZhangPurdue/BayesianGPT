Pruner:
  name: UnstructuredBayesainPruner
  params:
    sigma0: 1e-10
    sigma1: 0.1
    lambda_mix: 1e-7
    target_modules:
      - transformer.h.*.mlp.c_fc.weight
      - transformer.h.*.mlp.c_proj.weight
      - transformer.h.*.attn.c_attn.weight
      - transformer.h.*.attn.c_proj.weight

SparsityScheduler:
  name: CubicSparsityScheduler
  params:
    initial_sparsity: 0.1
    final_sparsity: 0.5
    total_train_steps: 1000
    pruning_start_step: 100
    pruning_end_step: 900
    pruning_interval: 100

Arguments:
  dataset_name: HuggingFaceFW/fineweb
  dataset_config_name: sample-10BT
  validation_split_percentage: 5
  model_name_or_path: openai-community/gpt2
  per_device_train_batch_size: 32
  learning_rate: 1e-5
  num_train_epochs: 1
  gradient_accumulation_steps: 1
  num_warmup_steps: 500
  seed: 42
  block_size: 1024
  preprocessing_num_workers: 32